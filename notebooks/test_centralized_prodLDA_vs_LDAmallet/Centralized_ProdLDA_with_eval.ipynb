{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sib1HSks6Xqv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.special import softmax\n",
        "import multiprocessing as mp\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CObHMSd6LLz"
      },
      "source": [
        "# Installing ProdLDA\n",
        "**Restart notbook after the installation!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VDAzxJA6FK5",
        "outputId": "46df1708-ac0f-482d-efda-d2bb6b6806d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PyTorchAVITM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/estebandito22/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6kW5jO66UKj"
      },
      "source": [
        "# 1. Creation of synthetic corpus\n",
        "\n",
        "We consider a scenario with n parties, each of them as an associated corpus.\n",
        "To generate the corpus associated with each of the parties, we consider a common beta distribution (word-topic distribution), but we freeze different topics/ assign different asymmetric Dirichlet priors favoring different topics at the time of generating the document that composes each party's corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twSZ3G0p6d1z"
      },
      "source": [
        "## 1.1. Function for permuting the Dirichlet prior at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tXdkpdrh6Thn"
      },
      "outputs": [],
      "source": [
        "def rotateArray(arr, n, d):\n",
        "    temp = []\n",
        "    i = 0\n",
        "    while (i < d):\n",
        "        temp.append(arr[i])\n",
        "        i = i + 1\n",
        "    i = 0\n",
        "    while (d < n):\n",
        "        arr[i] = arr[d]\n",
        "        i = i + 1\n",
        "        d = d + 1\n",
        "    arr[:] = arr[: i] + temp\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyFA9eGH6hGH"
      },
      "source": [
        "## 1.2. Topic modeling and node settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5DmfSiuR6iI0"
      },
      "outputs": [],
      "source": [
        "# Topic modeling settings\n",
        "vocab_size = 5000\n",
        "n_topics = 50\n",
        "beta = 1e-2\n",
        "alpha = 1/n_topics\n",
        "n_docs = 1000\n",
        "nwords = (150, 250) #Min and max lengths of the documents\n",
        "\n",
        "# Nodes settings\n",
        "n_nodes = 5\n",
        "frozen_topics = 5\n",
        "prior_frozen = frozen_topics * [alpha]\n",
        "own_topics = int((n_topics-frozen_topics)/n_nodes)\n",
        "prior_nofrozen = own_topics * [alpha] + (n_topics-frozen_topics-own_topics) * [alpha/10000]\n",
        "#print(prior_frozen + prior_nofrozen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ylo9Vsu6zpX"
      },
      "source": [
        "## 1.3. Topics generation (common for all nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy3AuOSx6qc1",
        "outputId": "08dc3a44-535e-488d-fa70-2e0547164e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades ordenadas para el primer vector de tópicos:\n",
            "[0.06343621 0.05713653 0.05458389 ... 0.         0.         0.        ]\n",
            "(50, 5000)\n"
          ]
        }
      ],
      "source": [
        "topic_vectors = np.random.dirichlet(vocab_size*[beta], n_topics)\n",
        "print('Probabilidades ordenadas para el primer vector de tópicos:')\n",
        "print(np.sort(topic_vectors[0])[::-1])\n",
        "print(topic_vectors.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1W6ldfZeJsT",
        "outputId": "b7b25314-6f37-43ae-a1a0-f5fb807aef2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tópicos (equivalentes) identificados correctamente (true): 50.00000000000006\n",
            "Tópicos (equivalentes) identificados correctamente (random): 3.660410001730459\n"
          ]
        }
      ],
      "source": [
        "#Here we compare alignment of the topic_vector matrix with itself and with another randomly generated matrix\n",
        "print('Tópicos (equivalentes) identificados correctamente (true):', np.sum(np.max(np.sqrt(topic_vectors).dot(np.sqrt(topic_vectors.T)), axis=0)))\n",
        "topic_vectors2 = np.random.dirichlet(vocab_size*[beta], n_topics)\n",
        "print('Tópicos (equivalentes) identificados correctamente (random):', np.sum(np.max(np.sqrt(topic_vectors2).dot(np.sqrt(topic_vectors.T)), axis=0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaQEe-yD6vl_"
      },
      "source": [
        "## 1.4. Generation of document topic proportions and documents for each node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VZJEmEhheNva"
      },
      "outputs": [],
      "source": [
        "# Step 2 - generation of document topic proportions\n",
        "doc_topics_all = []\n",
        "for i in np.arange(n_nodes):\n",
        "    doc_topics = np.random.dirichlet(prior_frozen + prior_nofrozen, n_docs)\n",
        "    prior_nofrozen = rotateArray(prior_nofrozen, len(prior_nofrozen), own_topics)\n",
        "    doc_topics_all.append(doc_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N-BziCW56vFL"
      },
      "outputs": [],
      "source": [
        "# Step 3 - Document generation\n",
        "documents_all = []\n",
        "z_all = []\n",
        "\n",
        "for i in np.arange(n_nodes):\n",
        "    documents = [] # Document words\n",
        "    #z = [] # Assignments\n",
        "    for docid in np.arange(n_docs):\n",
        "        doc_len = np.random.randint(low=nwords[0], high=nwords[1])\n",
        "        this_doc_words = []\n",
        "        #this_doc_assigns = []\n",
        "        for wd_idx in np.arange(doc_len):\n",
        "            tpc = np.nonzero(np.random.multinomial(1, doc_topics_all[i][docid]))[0][0]\n",
        "            #this_doc_assigns.append(tpc)\n",
        "            word = np.nonzero(np.random.multinomial(1, topic_vectors[tpc]))[0][0]\n",
        "            this_doc_words.append('wd'+str(word))\n",
        "        #z.append(this_doc_assigns)\n",
        "        documents.append(this_doc_words)\n",
        "    documents_all.append(documents)\n",
        "    #z_all.append(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XlZysxpSllMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbb6442-f0cd-4568-8726-82335eff459d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:719: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  val = np.asanyarray(val)\n"
          ]
        }
      ],
      "source": [
        "np.savez('synthetic_10000_beta_1.npz', n_nodes = n_nodes, vocab_size=vocab_size, n_topics=n_topics, frozen_topics = frozen_topics, beta=beta, alpha=alpha,\n",
        "        n_docs=n_docs, nwords=nwords, topic_vectors=topic_vectors, doc_topics=doc_topics_all,\n",
        "        documents=documents_all, z=z_all)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_topics_all_gt = doc_topics_all"
      ],
      "metadata": {
        "id": "Ru1gXdGUQ4en"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuJUlOIJ69iw"
      },
      "source": [
        "# 2. Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "14tnh0ndFpb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28592ad4-fd3c-40ef-951b-fcdb4f2c0ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM/pytorchavitm/datasets\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM/pytorchavitm/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "icRzu2ZE3naH"
      },
      "outputs": [],
      "source": [
        "from bow import BOWDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k4okSYycQaOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4ae092-3a5b-4caa-b3e6-9774d13e2b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gFPzE3yN3pLI"
      },
      "outputs": [],
      "source": [
        "from pytorchavitm import AVITM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(docs_train, docs_val):\n",
        "\n",
        "  # Get train-test split for each node and convert into AVITM format\n",
        "  cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                       max_df=0.99, min_df=0.01, binary=False)\n",
        "\n",
        "  # Generate AVITM dataset for correspinding node\n",
        "  docs_train_conv = [\" \".join(docs_train[i]) for i in np.arange(len(docs_train))]\n",
        "  train_bow = cv.fit_transform(docs_train_conv)\n",
        "  train_bow = train_bow.toarray()\n",
        "  idx2token = cv.get_feature_names()\n",
        "  input_size = len(idx2token)\n",
        "  id2token = {k: v for k, v in zip(range(0, len(idx2token)), idx2token)}\n",
        "  train_data = BOWDataset(train_bow, idx2token)\n",
        "\n",
        "  docs_val_conv = [\" \".join(docs_val[i]) for i in np.arange(len(docs_val))]\n",
        "  val_bow = cv.transform(docs_val_conv)\n",
        "  val_bow = val_bow.toarray()\n",
        "  val_data = BOWDataset(val_bow, idx2token)\n",
        "\n",
        "  return train_data, val_data, input_size, id2token"
      ],
      "metadata": {
        "id": "e95F4CkzxhIs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZGNArrP50AhG"
      },
      "outputs": [],
      "source": [
        "def train_avitm(docs_train, input_size_):\n",
        "  \"\"\"Trains an AVITM model with ProdLDA.\"\"\"   \n",
        "\n",
        "  avitm = AVITM(input_size=input_size_, n_components=n_topics, model_type='prodLDA',\n",
        "                hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "                learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "                solver='adam', num_epochs=100, reduce_on_plateau=True)\n",
        "  \n",
        "  avitm.fit(docs_train)\n",
        "\n",
        "  return avitm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_avitm(val_data, avtim, k):\n",
        "  avtim.model.eval()\n",
        "  \n",
        "  loader = DataLoader(\n",
        "      val_data, batch_size=avtim.batch_size, shuffle=False,\n",
        "      num_workers=mp.cpu_count())\n",
        "\n",
        "  preds = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_samples in loader:\n",
        "      # batch_size x vocab_size\n",
        "      X = batch_samples['X']\n",
        "\n",
        "      if avtim.USE_CUDA:\n",
        "        X = X.cuda()\n",
        "\n",
        "      # forward pass\n",
        "      avtim.model.zero_grad()\n",
        "      values = avtim.model(X)\n",
        "      word_dists = values[-1]\n",
        "\n",
        "      _, indices = torch.sort(word_dists, dim=1)\n",
        "      preds += [indices[:, :k]]\n",
        "\n",
        "    preds = torch.cat(preds, dim=0)\n",
        "\n",
        "  n_samples = 20\n",
        "  pbar = tqdm(n_samples, position=0, leave=True)\n",
        "\n",
        "  pred_thetas = []\n",
        "  for sample_index in range(n_samples):\n",
        "    with torch.no_grad():\n",
        "      collect_theta = []\n",
        "\n",
        "      for batch_samples in loader:\n",
        "        X = batch_samples['X']\n",
        "\n",
        "        if avtim.USE_CUDA:\n",
        "          X = X.cuda()\n",
        "\n",
        "        # forward pass\n",
        "        avtim.model.zero_grad()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          posterior_mu, posterior_log_sigma = avtim.model.inf_net(X)\n",
        "\n",
        "          # Generate samples from theta\n",
        "          theta = F.softmax(\n",
        "                  avtim.model.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
        "          theta = avtim.model.drop_theta(theta)\n",
        "\n",
        "        collect_theta.extend(theta.cpu().numpy().tolist())\n",
        "\n",
        "      pbar.update(1)\n",
        "      pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
        "\n",
        "      pred_thetas.append(np.array(collect_theta))\n",
        "          \n",
        "  pbar.close()\n",
        "  pred_thetas = np.sum(pred_thetas, axis=0) / n_samples\n",
        "\n",
        "  return preds,pred_thetas"
      ],
      "metadata": {
        "id": "2bua2Wpp4_fw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "orpK--hqxQkG"
      },
      "outputs": [],
      "source": [
        "def get_doc_topic_distribution(avitm, dataset, n_samples=20):\n",
        "    \"\"\"Given a trained AVITM model, it gets its associated document-topic distribution.\n",
        "\n",
        "    Args:\n",
        "        * n_samples (int, optional): Defaults to 20.\n",
        "\n",
        "    Returns:\n",
        "        * ndarray : Document-topics distribution\n",
        "    \"\"\"     \n",
        "    avitm.model.eval()\n",
        "\n",
        "    loader = DataLoader(\n",
        "            avitm.train_data, batch_size=avitm.batch_size, shuffle=True,\n",
        "            num_workers=mp.cpu_count())\n",
        "\n",
        "    pbar = tqdm(n_samples, position=0, leave=True)\n",
        "\n",
        "    final_thetas = []\n",
        "    for sample_index in range(n_samples):\n",
        "        with torch.no_grad():\n",
        "            collect_theta = []\n",
        "\n",
        "            for batch_samples in loader:\n",
        "                X = batch_samples['X']\n",
        "\n",
        "                if avitm.USE_CUDA:\n",
        "                  X = X.cuda()\n",
        "\n",
        "                # forward pass\n",
        "                avitm.model.zero_grad()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                  posterior_mu, posterior_log_sigma = avitm.model.inf_net(X)\n",
        "\n",
        "                  # Generate samples from theta\n",
        "                  theta = F.softmax(\n",
        "                          avitm.model.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
        "                  theta = avitm.model.drop_theta(theta)\n",
        "\n",
        "                collect_theta.extend(theta.cpu().numpy().tolist())\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
        "\n",
        "            final_thetas.append(np.array(collect_theta))\n",
        "    pbar.close()\n",
        "    return np.sum(final_thetas, axis=0) / n_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "douV1llyTM4b"
      },
      "outputs": [],
      "source": [
        "def get_topic_word_distribution(avtim_model):\n",
        "  \"\"\"Given a trained AVITM model, it gets its associated topic-word distribution.\n",
        "\n",
        "    Args:\n",
        "        * avtim_model (AVITM): Trained AVITM model.\n",
        "\n",
        "    Returns:\n",
        "        * ndarray : topic-word distribution\n",
        "    \"\"\"     \n",
        "  topic_word_matrix = avtim_model.best_components.cpu().detach().numpy()\n",
        "  wd = softmax(topic_word_matrix, axis=1)\n",
        "  return normalize(wd,axis=1,norm='l1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CT-rniqA78QO"
      },
      "outputs": [],
      "source": [
        "def convert_topic_word_to_init_size(vocab_size, model, ntopics,\n",
        "                                    id2token, all_words):\n",
        "    \"\"\"It converts the topic-word distribution matrix obtained from the\n",
        "    training of a model into a matrix with the dimensions of the original\n",
        "    topic-word distribution, assigning zeros to those words that are not\n",
        "    present in the corpus. \n",
        "    It is only of use in case we are training a model over a synthetic dataset,\n",
        "    so as to later compare the performance of the attained model in what regards\n",
        "    to the similarity between the original and the trained model.\n",
        "\n",
        "    Args:\n",
        "        * vocab_size (int):       Size of the synethic'data vocabulary.\n",
        "        * model (AVITM):          Model whose topic-word matrix is being transformed.\n",
        "        * ntopics (int):          Number of topics of the trained model.\n",
        "        * id2token (List[tuple]): Mappings with the content of the document-term matrix.\n",
        "        * all_words (List[str]):  List of all the words of the vocabulary of size vocab_size.\n",
        "\n",
        "    Returns:\n",
        "        * ndarray: Normalized transormed topic-word distribution.\n",
        "    \"\"\"\n",
        "    w_t_distrib = np.zeros((ntopics, vocab_size), dtype=np.float64)\n",
        "    wd = get_topic_word_distribution(model)\n",
        "    for i in np.arange(ntopics):\n",
        "        for idx, word in id2token.items():\n",
        "            for j in np.arange(len(all_words)):\n",
        "                if all_words[j] == word:\n",
        "                    w_t_distrib[i,j] = wd[i][idx]\n",
        "                    break\n",
        "    return w_t_distrib\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Generate datasets"
      ],
      "metadata": {
        "id": "r0gRi0cyPcaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Train and validations datasets for the node 0"
      ],
      "metadata": {
        "id": "fFWsS5XzcTO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get documents and document-topic proportions associated to the node 0\n",
        "cut = int(len(documents_all[0])/4)\n",
        "cut = 3*cut\n",
        "docs_node_train = documents_all[0][0:cut]\n",
        "docs_node_val = documents_all[0][cut:]\n",
        "doc_topics_train_0 = doc_topics_all[0][0:cut,:]\n",
        "doc_topics_val_0 = doc_topics_all[0][cut:,:]\n",
        "\n",
        "# Prepare node 0's dataset in AVITM format\n",
        "documents_train_0, documents_val_0, input_size_0, id2token_0 = prepare_dataset(docs_node_train, docs_node_val)"
      ],
      "metadata": {
        "id": "f4gMBYJ9Pf4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731c6b92-b917-4327-d1ef-d355a8d9f940"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Train and validation datasets for the centralized model"
      ],
      "metadata": {
        "id": "aI0JQUhJcYpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get documents and document-topic proportions for the centralized model\n",
        "\n",
        "# Divide between train and val docs for each node \n",
        "docs_centr_train = []\n",
        "docs_centr_val = []\n",
        "doc_topics_train_centr = []\n",
        "doc_topics_val_centr = []\n",
        "for node in range(n_nodes):\n",
        "  docs_centr_train.append(documents_all[node][0:cut])\n",
        "  docs_centr_val.append(documents_all[node][cut:])\n",
        "  doc_topics_train_centr.append(doc_topics_all[node][0:cut,:])\n",
        "  doc_topics_val_centr.append(doc_topics_all[node][cut:,:])\n",
        "\n",
        "# Collapse all centralized documents into one list for training and another one for validation.\n",
        "# Same applyes for the doc-tops, but they are concatenated into arrays\n",
        "documents_centr_train = [*docs_centr_train[0], *docs_centr_train[1], *docs_centr_train[2], *docs_centr_train[3], *docs_centr_train[4]]\n",
        "documents_centr_val = [*docs_centr_val[0], *docs_centr_val[1], *docs_centr_val[2], *docs_centr_val[3], *docs_centr_val[4]]\n",
        "doc_topics_train_centr = np.concatenate((doc_topics_train_centr[0], doc_topics_train_centr[1], doc_topics_train_centr[2], doc_topics_train_centr[3], doc_topics_train_centr[4]), axis=0)\n",
        "doc_topics_val_centr = np.concatenate((doc_topics_val_centr[0], doc_topics_val_centr[1], doc_topics_val_centr[2], doc_topics_val_centr[3], doc_topics_val_centr[4]), axis=0)\n",
        "\n",
        "# Prepare centralized dataset in AVITM format\n",
        "documents_train_centr, documents_val_centr, input_size_centr, id2token_centr = prepare_dataset(documents_centr_train, documents_centr_val)"
      ],
      "metadata": {
        "id": "Gbj3-6Uac7YZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066f7603-1ed9-4f90-c49c-669cd27d7436"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QieP5DdU7zY0"
      },
      "source": [
        "# 3. Centralized ProdLDA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kxW_tMtFRyfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9284ceff-a47f-4cc6-951d-86a9b52c1a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 50\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.98\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: True\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [3750/375000]\tTrain Loss: 1528.4835270833332\tTime: 0:00:00.995303\n",
            "Epoch: [2/100]\tSamples: [7500/375000]\tTrain Loss: 1396.4731166666666\tTime: 0:00:00.977431\n",
            "Epoch: [3/100]\tSamples: [11250/375000]\tTrain Loss: 1347.6133635416666\tTime: 0:00:00.968912\n",
            "Epoch: [4/100]\tSamples: [15000/375000]\tTrain Loss: 1308.5852833333333\tTime: 0:00:00.968127\n",
            "Epoch: [5/100]\tSamples: [18750/375000]\tTrain Loss: 1283.800190625\tTime: 0:00:00.979791\n",
            "Epoch: [6/100]\tSamples: [22500/375000]\tTrain Loss: 1262.3216604166666\tTime: 0:00:00.936936\n",
            "Epoch: [7/100]\tSamples: [26250/375000]\tTrain Loss: 1245.538146875\tTime: 0:00:00.931391\n",
            "Epoch: [8/100]\tSamples: [30000/375000]\tTrain Loss: 1231.31045\tTime: 0:00:00.917413\n",
            "Epoch: [9/100]\tSamples: [33750/375000]\tTrain Loss: 1225.3853270833333\tTime: 0:00:00.970075\n",
            "Epoch: [10/100]\tSamples: [37500/375000]\tTrain Loss: 1217.8804604166667\tTime: 0:00:00.957040\n",
            "Epoch: [11/100]\tSamples: [41250/375000]\tTrain Loss: 1208.9420395833333\tTime: 0:00:00.975501\n",
            "Epoch: [12/100]\tSamples: [45000/375000]\tTrain Loss: 1204.163546875\tTime: 0:00:00.951727\n",
            "Epoch: [13/100]\tSamples: [48750/375000]\tTrain Loss: 1195.1951489583332\tTime: 0:00:00.960436\n",
            "Epoch: [14/100]\tSamples: [52500/375000]\tTrain Loss: 1201.7280802083333\tTime: 0:00:00.930634\n",
            "Epoch: [15/100]\tSamples: [56250/375000]\tTrain Loss: 1193.8823010416668\tTime: 0:00:00.954594\n",
            "Epoch: [16/100]\tSamples: [60000/375000]\tTrain Loss: 1189.0386302083334\tTime: 0:00:00.936971\n",
            "Epoch: [17/100]\tSamples: [63750/375000]\tTrain Loss: 1189.619653125\tTime: 0:00:00.944159\n",
            "Epoch: [18/100]\tSamples: [67500/375000]\tTrain Loss: 1190.86165625\tTime: 0:00:00.945929\n",
            "Epoch: [19/100]\tSamples: [71250/375000]\tTrain Loss: 1186.91300625\tTime: 0:00:00.928072\n",
            "Epoch: [20/100]\tSamples: [75000/375000]\tTrain Loss: 1185.202125\tTime: 0:00:00.938368\n",
            "Epoch: [21/100]\tSamples: [78750/375000]\tTrain Loss: 1184.2426729166666\tTime: 0:00:00.943077\n",
            "Epoch: [22/100]\tSamples: [82500/375000]\tTrain Loss: 1184.349553125\tTime: 0:00:00.959691\n",
            "Epoch: [23/100]\tSamples: [86250/375000]\tTrain Loss: 1179.77249375\tTime: 0:00:00.961983\n",
            "Epoch: [24/100]\tSamples: [90000/375000]\tTrain Loss: 1179.254790625\tTime: 0:00:00.939117\n",
            "Epoch: [25/100]\tSamples: [93750/375000]\tTrain Loss: 1178.6488791666666\tTime: 0:00:00.967648\n",
            "Epoch: [26/100]\tSamples: [97500/375000]\tTrain Loss: 1177.2605166666667\tTime: 0:00:00.976331\n",
            "Epoch: [27/100]\tSamples: [101250/375000]\tTrain Loss: 1177.0798583333333\tTime: 0:00:00.949888\n",
            "Epoch: [28/100]\tSamples: [105000/375000]\tTrain Loss: 1174.7460395833334\tTime: 0:00:00.947526\n",
            "Epoch: [29/100]\tSamples: [108750/375000]\tTrain Loss: 1175.7482541666666\tTime: 0:00:00.943015\n",
            "Epoch: [30/100]\tSamples: [112500/375000]\tTrain Loss: 1174.99416875\tTime: 0:00:00.982890\n",
            "Epoch: [31/100]\tSamples: [116250/375000]\tTrain Loss: 1174.475821875\tTime: 0:00:00.964248\n",
            "Epoch: [32/100]\tSamples: [120000/375000]\tTrain Loss: 1177.1775520833332\tTime: 0:00:00.929115\n",
            "Epoch: [33/100]\tSamples: [123750/375000]\tTrain Loss: 1170.713034375\tTime: 0:00:00.969349\n",
            "Epoch: [34/100]\tSamples: [127500/375000]\tTrain Loss: 1169.8928052083334\tTime: 0:00:00.964154\n",
            "Epoch: [35/100]\tSamples: [131250/375000]\tTrain Loss: 1172.8023791666667\tTime: 0:00:00.970785\n",
            "Epoch: [36/100]\tSamples: [135000/375000]\tTrain Loss: 1174.3771614583334\tTime: 0:00:00.945304\n",
            "Epoch: [37/100]\tSamples: [138750/375000]\tTrain Loss: 1171.0128875\tTime: 0:00:00.951434\n",
            "Epoch: [38/100]\tSamples: [142500/375000]\tTrain Loss: 1168.0098927083334\tTime: 0:00:00.946121\n",
            "Epoch: [39/100]\tSamples: [146250/375000]\tTrain Loss: 1170.3878583333333\tTime: 0:00:00.941078\n",
            "Epoch: [40/100]\tSamples: [150000/375000]\tTrain Loss: 1172.628353125\tTime: 0:00:00.959014\n",
            "Epoch: [41/100]\tSamples: [153750/375000]\tTrain Loss: 1167.4254541666667\tTime: 0:00:00.937450\n",
            "Epoch: [42/100]\tSamples: [157500/375000]\tTrain Loss: 1171.19874375\tTime: 0:00:00.949115\n",
            "Epoch: [43/100]\tSamples: [161250/375000]\tTrain Loss: 1165.4157729166666\tTime: 0:00:00.925417\n",
            "Epoch: [44/100]\tSamples: [165000/375000]\tTrain Loss: 1169.7278927083332\tTime: 0:00:00.945334\n",
            "Epoch: [45/100]\tSamples: [168750/375000]\tTrain Loss: 1168.0180229166667\tTime: 0:00:00.963560\n",
            "Epoch: [46/100]\tSamples: [172500/375000]\tTrain Loss: 1166.6628854166668\tTime: 0:00:00.963606\n",
            "Epoch: [47/100]\tSamples: [176250/375000]\tTrain Loss: 1171.6427916666667\tTime: 0:00:00.959899\n",
            "Epoch: [48/100]\tSamples: [180000/375000]\tTrain Loss: 1166.760740625\tTime: 0:00:00.952950\n",
            "Epoch: [49/100]\tSamples: [183750/375000]\tTrain Loss: 1166.945540625\tTime: 0:00:00.944048\n",
            "Epoch: [50/100]\tSamples: [187500/375000]\tTrain Loss: 1167.6922729166668\tTime: 0:00:00.947681\n",
            "Epoch: [51/100]\tSamples: [191250/375000]\tTrain Loss: 1169.4515260416667\tTime: 0:00:00.944165\n",
            "Epoch: [52/100]\tSamples: [195000/375000]\tTrain Loss: 1172.950828125\tTime: 0:00:00.949985\n",
            "Epoch: [53/100]\tSamples: [198750/375000]\tTrain Loss: 1170.71334375\tTime: 0:00:00.958052\n",
            "Epoch: [54/100]\tSamples: [202500/375000]\tTrain Loss: 1167.2760708333333\tTime: 0:00:00.976387\n",
            "Epoch: [55/100]\tSamples: [206250/375000]\tTrain Loss: 1170.4715791666667\tTime: 0:00:00.955424\n",
            "Epoch: [56/100]\tSamples: [210000/375000]\tTrain Loss: 1171.4933177083333\tTime: 0:00:00.961645\n",
            "Epoch: [57/100]\tSamples: [213750/375000]\tTrain Loss: 1165.7862625\tTime: 0:00:00.951625\n",
            "Epoch: [58/100]\tSamples: [217500/375000]\tTrain Loss: 1160.8811697916667\tTime: 0:00:00.929377\n",
            "Epoch: [59/100]\tSamples: [221250/375000]\tTrain Loss: 1164.35439375\tTime: 0:00:00.963988\n",
            "Epoch: [60/100]\tSamples: [225000/375000]\tTrain Loss: 1167.86098125\tTime: 0:00:00.947420\n",
            "Epoch: [61/100]\tSamples: [228750/375000]\tTrain Loss: 1162.4792458333334\tTime: 0:00:00.938061\n",
            "Epoch: [62/100]\tSamples: [232500/375000]\tTrain Loss: 1168.32106875\tTime: 0:00:00.939658\n",
            "Epoch: [63/100]\tSamples: [236250/375000]\tTrain Loss: 1164.3160208333334\tTime: 0:00:00.954507\n",
            "Epoch: [64/100]\tSamples: [240000/375000]\tTrain Loss: 1167.7726958333333\tTime: 0:00:00.947801\n",
            "Epoch: [65/100]\tSamples: [243750/375000]\tTrain Loss: 1161.4116208333332\tTime: 0:00:00.970758\n",
            "Epoch: [66/100]\tSamples: [247500/375000]\tTrain Loss: 1161.1437270833333\tTime: 0:00:00.979283\n",
            "Epoch: [67/100]\tSamples: [251250/375000]\tTrain Loss: 1159.9139\tTime: 0:00:00.959558\n",
            "Epoch: [68/100]\tSamples: [255000/375000]\tTrain Loss: 1163.36511875\tTime: 0:00:00.962558\n",
            "Epoch: [69/100]\tSamples: [258750/375000]\tTrain Loss: 1161.9457552083334\tTime: 0:00:00.977361\n",
            "Epoch: [70/100]\tSamples: [262500/375000]\tTrain Loss: 1162.46784375\tTime: 0:00:00.940859\n",
            "Epoch: [71/100]\tSamples: [266250/375000]\tTrain Loss: 1164.6358385416668\tTime: 0:00:00.943714\n",
            "Epoch: [72/100]\tSamples: [270000/375000]\tTrain Loss: 1163.2952385416668\tTime: 0:00:00.959957\n",
            "Epoch: [73/100]\tSamples: [273750/375000]\tTrain Loss: 1163.4313354166666\tTime: 0:00:00.987214\n",
            "Epoch: [74/100]\tSamples: [277500/375000]\tTrain Loss: 1166.8395520833333\tTime: 0:00:00.969686\n",
            "Epoch: [75/100]\tSamples: [281250/375000]\tTrain Loss: 1163.82689375\tTime: 0:00:00.954550\n",
            "Epoch: [76/100]\tSamples: [285000/375000]\tTrain Loss: 1163.5855770833334\tTime: 0:00:00.941619\n",
            "Epoch: [77/100]\tSamples: [288750/375000]\tTrain Loss: 1165.9970635416666\tTime: 0:00:00.943171\n",
            "Epoch: [78/100]\tSamples: [292500/375000]\tTrain Loss: 1160.189\tTime: 0:00:00.950999\n",
            "Epoch: [79/100]\tSamples: [296250/375000]\tTrain Loss: 1165.7728833333333\tTime: 0:00:00.952904\n",
            "Epoch: [80/100]\tSamples: [300000/375000]\tTrain Loss: 1165.1534020833333\tTime: 0:00:00.961646\n",
            "Epoch: [81/100]\tSamples: [303750/375000]\tTrain Loss: 1163.2974489583332\tTime: 0:00:00.945609\n",
            "Epoch: [82/100]\tSamples: [307500/375000]\tTrain Loss: 1160.428684375\tTime: 0:00:00.975551\n",
            "Epoch: [83/100]\tSamples: [311250/375000]\tTrain Loss: 1162.4449604166666\tTime: 0:00:00.979078\n",
            "Epoch: [84/100]\tSamples: [315000/375000]\tTrain Loss: 1164.0660760416667\tTime: 0:00:00.976083\n",
            "Epoch: [85/100]\tSamples: [318750/375000]\tTrain Loss: 1164.7937697916666\tTime: 0:00:00.996493\n",
            "Epoch: [86/100]\tSamples: [322500/375000]\tTrain Loss: 1165.0222645833333\tTime: 0:00:00.934357\n",
            "Epoch: [87/100]\tSamples: [326250/375000]\tTrain Loss: 1162.2943291666666\tTime: 0:00:00.958801\n",
            "Epoch: [88/100]\tSamples: [330000/375000]\tTrain Loss: 1163.5691208333333\tTime: 0:00:00.951170\n",
            "Epoch: [89/100]\tSamples: [333750/375000]\tTrain Loss: 1159.7846770833332\tTime: 0:00:00.979767\n",
            "Epoch: [90/100]\tSamples: [337500/375000]\tTrain Loss: 1158.6073739583333\tTime: 0:00:00.959630\n",
            "Epoch: [91/100]\tSamples: [341250/375000]\tTrain Loss: 1161.0196145833334\tTime: 0:00:00.975165\n",
            "Epoch: [92/100]\tSamples: [345000/375000]\tTrain Loss: 1161.2471645833334\tTime: 0:00:00.975291\n",
            "Epoch: [93/100]\tSamples: [348750/375000]\tTrain Loss: 1159.9180416666666\tTime: 0:00:00.966376\n",
            "Epoch: [94/100]\tSamples: [352500/375000]\tTrain Loss: 1162.12868125\tTime: 0:00:00.977186\n",
            "Epoch: [95/100]\tSamples: [356250/375000]\tTrain Loss: 1160.2715916666666\tTime: 0:00:00.957852\n",
            "Epoch: [96/100]\tSamples: [360000/375000]\tTrain Loss: 1162.5853291666667\tTime: 0:00:00.956631\n",
            "Epoch: [97/100]\tSamples: [363750/375000]\tTrain Loss: 1158.8746020833332\tTime: 0:00:00.976982\n",
            "Epoch: [98/100]\tSamples: [367500/375000]\tTrain Loss: 1162.9437864583333\tTime: 0:00:00.970089\n",
            "Epoch: [99/100]\tSamples: [371250/375000]\tTrain Loss: 1160.6248875\tTime: 0:00:00.972505\n",
            "Epoch: [100/100]\tSamples: [375000/375000]\tTrain Loss: 1160.0188635416666\tTime: 0:00:00.984104\n"
          ]
        }
      ],
      "source": [
        "avitm_centr = train_avitm(documents_train_centr, input_size_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Z1w7cb9r7QAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8619a79-d33f-426a-d3df-93b62a03300a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:02,  2.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3750, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic_centr = get_doc_topic_distribution(avitm_centr, documents_train_centr, n_samples=5) # get all the topic predictions\n",
        "print(doc_topic_centr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = ['wd'+str(word) for word in np.arange(vocab_size+1) if word > 0]"
      ],
      "metadata": {
        "id": "JzD82O09Lw2M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YHry56Bz7sbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a347edd4-5947-4154-86d5-a1ea3291bac9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "word_topic_centr = convert_topic_word_to_init_size(vocab_size, avitm_centr, n_topics, id2token_centr, all_words)\n",
        "word_topic_centr.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_centr, pred_thetas_centr = eval_avitm(documents_val_centr, avitm_centr, 50)\n",
        "print(pred_thetas_centr)"
      ],
      "metadata": {
        "id": "1Rp_9KZ0MV-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a6df84-6efc-4f26-ed81-5e754e8a0760"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [20/20]: : 20it [00:04,  4.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.01147759 0.01327403 0.01104251 ... 0.00935853 0.00835784 0.01697652]\n",
            " [0.0075569  0.01239275 0.0103828  ... 0.0553876  0.01547357 0.01014435]\n",
            " [0.00678008 0.00623024 0.00678342 ... 0.0130512  0.01041772 0.00788356]\n",
            " ...\n",
            " [0.01820842 0.00757589 0.0117983  ... 0.00814694 0.01093852 0.00708112]\n",
            " [0.00453051 0.00746715 0.0038398  ... 0.00917913 0.01110446 0.00441258]\n",
            " [0.01358224 0.00736376 0.01328202 ... 0.01115559 0.00442354 0.00213456]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Node 0"
      ],
      "metadata": {
        "id": "IDiJxOUaMz5G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wchhyQ5bDIhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55bfec20-8953-437f-95c0-356863259001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 50\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.98\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: True\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [750/75000]\tTrain Loss: 1528.421875\tTime: 0:00:00.208241\n",
            "Epoch: [2/100]\tSamples: [1500/75000]\tTrain Loss: 1464.0140520833334\tTime: 0:00:00.230494\n",
            "Epoch: [3/100]\tSamples: [2250/75000]\tTrain Loss: 1403.3408229166666\tTime: 0:00:00.236690\n",
            "Epoch: [4/100]\tSamples: [3000/75000]\tTrain Loss: 1356.2418125\tTime: 0:00:00.218632\n",
            "Epoch: [5/100]\tSamples: [3750/75000]\tTrain Loss: 1322.8078645833334\tTime: 0:00:00.207862\n",
            "Epoch: [6/100]\tSamples: [4500/75000]\tTrain Loss: 1291.7146770833333\tTime: 0:00:00.227724\n",
            "Epoch: [7/100]\tSamples: [5250/75000]\tTrain Loss: 1271.90865625\tTime: 0:00:00.216613\n",
            "Epoch: [8/100]\tSamples: [6000/75000]\tTrain Loss: 1262.1562395833334\tTime: 0:00:00.232120\n",
            "Epoch: [9/100]\tSamples: [6750/75000]\tTrain Loss: 1246.779671875\tTime: 0:00:00.224473\n",
            "Epoch: [10/100]\tSamples: [7500/75000]\tTrain Loss: 1234.7581354166666\tTime: 0:00:00.213576\n",
            "Epoch: [11/100]\tSamples: [8250/75000]\tTrain Loss: 1227.1871458333333\tTime: 0:00:00.228348\n",
            "Epoch: [12/100]\tSamples: [9000/75000]\tTrain Loss: 1212.0644166666666\tTime: 0:00:00.220080\n",
            "Epoch: [13/100]\tSamples: [9750/75000]\tTrain Loss: 1209.05521875\tTime: 0:00:00.211107\n",
            "Epoch: [14/100]\tSamples: [10500/75000]\tTrain Loss: 1206.39084375\tTime: 0:00:00.212773\n",
            "Epoch: [15/100]\tSamples: [11250/75000]\tTrain Loss: 1197.758265625\tTime: 0:00:00.229155\n",
            "Epoch: [16/100]\tSamples: [12000/75000]\tTrain Loss: 1193.3480625\tTime: 0:00:00.208994\n",
            "Epoch: [17/100]\tSamples: [12750/75000]\tTrain Loss: 1191.5169791666667\tTime: 0:00:00.226220\n",
            "Epoch: [18/100]\tSamples: [13500/75000]\tTrain Loss: 1185.4422291666667\tTime: 0:00:00.213830\n",
            "Epoch: [19/100]\tSamples: [14250/75000]\tTrain Loss: 1185.4646666666667\tTime: 0:00:00.211260\n",
            "Epoch: [20/100]\tSamples: [15000/75000]\tTrain Loss: 1178.4687604166666\tTime: 0:00:00.223216\n",
            "Epoch: [21/100]\tSamples: [15750/75000]\tTrain Loss: 1175.1085833333334\tTime: 0:00:00.224892\n",
            "Epoch: [22/100]\tSamples: [16500/75000]\tTrain Loss: 1172.991265625\tTime: 0:00:00.225154\n",
            "Epoch: [23/100]\tSamples: [17250/75000]\tTrain Loss: 1168.1528541666667\tTime: 0:00:00.212393\n",
            "Epoch: [24/100]\tSamples: [18000/75000]\tTrain Loss: 1173.279953125\tTime: 0:00:00.215875\n",
            "Epoch: [25/100]\tSamples: [18750/75000]\tTrain Loss: 1161.7799322916667\tTime: 0:00:00.219137\n",
            "Epoch: [26/100]\tSamples: [19500/75000]\tTrain Loss: 1158.525203125\tTime: 0:00:00.226251\n",
            "Epoch: [27/100]\tSamples: [20250/75000]\tTrain Loss: 1162.17203125\tTime: 0:00:00.208286\n",
            "Epoch: [28/100]\tSamples: [21000/75000]\tTrain Loss: 1154.6668385416667\tTime: 0:00:00.213315\n",
            "Epoch: [29/100]\tSamples: [21750/75000]\tTrain Loss: 1155.1379375\tTime: 0:00:00.228716\n",
            "Epoch: [30/100]\tSamples: [22500/75000]\tTrain Loss: 1152.932671875\tTime: 0:00:00.204996\n",
            "Epoch: [31/100]\tSamples: [23250/75000]\tTrain Loss: 1155.201515625\tTime: 0:00:00.224327\n",
            "Epoch: [32/100]\tSamples: [24000/75000]\tTrain Loss: 1151.124171875\tTime: 0:00:00.217406\n",
            "Epoch: [33/100]\tSamples: [24750/75000]\tTrain Loss: 1149.7399479166668\tTime: 0:00:00.216461\n",
            "Epoch: [34/100]\tSamples: [25500/75000]\tTrain Loss: 1150.2478125\tTime: 0:00:00.218214\n",
            "Epoch: [35/100]\tSamples: [26250/75000]\tTrain Loss: 1148.394734375\tTime: 0:00:00.227761\n",
            "Epoch: [36/100]\tSamples: [27000/75000]\tTrain Loss: 1142.0900104166667\tTime: 0:00:00.243915\n",
            "Epoch: [37/100]\tSamples: [27750/75000]\tTrain Loss: 1147.837046875\tTime: 0:00:00.211723\n",
            "Epoch: [38/100]\tSamples: [28500/75000]\tTrain Loss: 1148.38265625\tTime: 0:00:00.214565\n",
            "Epoch: [39/100]\tSamples: [29250/75000]\tTrain Loss: 1146.8055625\tTime: 0:00:00.212462\n",
            "Epoch: [40/100]\tSamples: [30000/75000]\tTrain Loss: 1136.5064791666666\tTime: 0:00:00.219700\n",
            "Epoch: [41/100]\tSamples: [30750/75000]\tTrain Loss: 1144.2085260416666\tTime: 0:00:00.215210\n",
            "Epoch: [42/100]\tSamples: [31500/75000]\tTrain Loss: 1142.8735364583333\tTime: 0:00:00.214154\n",
            "Epoch: [43/100]\tSamples: [32250/75000]\tTrain Loss: 1140.0700104166667\tTime: 0:00:00.225423\n",
            "Epoch: [44/100]\tSamples: [33000/75000]\tTrain Loss: 1142.382546875\tTime: 0:00:00.225357\n",
            "Epoch: [45/100]\tSamples: [33750/75000]\tTrain Loss: 1141.4012083333334\tTime: 0:00:00.223217\n",
            "Epoch: [46/100]\tSamples: [34500/75000]\tTrain Loss: 1140.0203489583334\tTime: 0:00:00.215874\n",
            "Epoch: [47/100]\tSamples: [35250/75000]\tTrain Loss: 1138.9121510416667\tTime: 0:00:00.218143\n",
            "Epoch: [48/100]\tSamples: [36000/75000]\tTrain Loss: 1141.8214895833332\tTime: 0:00:00.210777\n",
            "Epoch: [49/100]\tSamples: [36750/75000]\tTrain Loss: 1138.3458854166668\tTime: 0:00:00.214483\n",
            "Epoch: [50/100]\tSamples: [37500/75000]\tTrain Loss: 1140.1701458333334\tTime: 0:00:00.222824\n",
            "Epoch: [51/100]\tSamples: [38250/75000]\tTrain Loss: 1137.51084375\tTime: 0:00:00.217014\n",
            "Epoch: [52/100]\tSamples: [39000/75000]\tTrain Loss: 1138.3510416666666\tTime: 0:00:00.228667\n",
            "Epoch: [53/100]\tSamples: [39750/75000]\tTrain Loss: 1134.19575\tTime: 0:00:00.237726\n",
            "Epoch: [54/100]\tSamples: [40500/75000]\tTrain Loss: 1136.1815625\tTime: 0:00:00.233526\n",
            "Epoch: [55/100]\tSamples: [41250/75000]\tTrain Loss: 1136.051546875\tTime: 0:00:00.208453\n",
            "Epoch: [56/100]\tSamples: [42000/75000]\tTrain Loss: 1136.16315625\tTime: 0:00:00.218217\n",
            "Epoch: [57/100]\tSamples: [42750/75000]\tTrain Loss: 1141.0706041666667\tTime: 0:00:00.219011\n",
            "Epoch: [58/100]\tSamples: [43500/75000]\tTrain Loss: 1133.8255989583333\tTime: 0:00:00.215061\n",
            "Epoch: [59/100]\tSamples: [44250/75000]\tTrain Loss: 1131.5167760416666\tTime: 0:00:00.218024\n",
            "Epoch: [60/100]\tSamples: [45000/75000]\tTrain Loss: 1128.3107864583333\tTime: 0:00:00.207980\n",
            "Epoch: [61/100]\tSamples: [45750/75000]\tTrain Loss: 1136.4611822916668\tTime: 0:00:00.219550\n",
            "Epoch: [62/100]\tSamples: [46500/75000]\tTrain Loss: 1135.5672083333334\tTime: 0:00:00.220876\n",
            "Epoch: [63/100]\tSamples: [47250/75000]\tTrain Loss: 1131.84334375\tTime: 0:00:00.223628\n",
            "Epoch: [64/100]\tSamples: [48000/75000]\tTrain Loss: 1137.9981927083334\tTime: 0:00:00.235906\n",
            "Epoch: [65/100]\tSamples: [48750/75000]\tTrain Loss: 1125.6093854166666\tTime: 0:00:00.220394\n",
            "Epoch: [66/100]\tSamples: [49500/75000]\tTrain Loss: 1128.829140625\tTime: 0:00:00.204728\n",
            "Epoch: [67/100]\tSamples: [50250/75000]\tTrain Loss: 1135.1272708333333\tTime: 0:00:00.212665\n",
            "Epoch: [68/100]\tSamples: [51000/75000]\tTrain Loss: 1127.546453125\tTime: 0:00:00.216232\n",
            "Epoch: [69/100]\tSamples: [51750/75000]\tTrain Loss: 1129.93665625\tTime: 0:00:00.206490\n",
            "Epoch: [70/100]\tSamples: [52500/75000]\tTrain Loss: 1128.8763125\tTime: 0:00:00.215549\n",
            "Epoch: [71/100]\tSamples: [53250/75000]\tTrain Loss: 1133.046328125\tTime: 0:00:00.221972\n",
            "Epoch: [72/100]\tSamples: [54000/75000]\tTrain Loss: 1128.7438958333332\tTime: 0:00:00.217048\n",
            "Epoch: [73/100]\tSamples: [54750/75000]\tTrain Loss: 1125.258484375\tTime: 0:00:00.226420\n",
            "Epoch: [74/100]\tSamples: [55500/75000]\tTrain Loss: 1133.0834791666666\tTime: 0:00:00.229207\n",
            "Epoch: [75/100]\tSamples: [56250/75000]\tTrain Loss: 1123.925796875\tTime: 0:00:00.224203\n",
            "Epoch: [76/100]\tSamples: [57000/75000]\tTrain Loss: 1129.407671875\tTime: 0:00:00.220739\n",
            "Epoch: [77/100]\tSamples: [57750/75000]\tTrain Loss: 1123.3520833333334\tTime: 0:00:00.219688\n",
            "Epoch: [78/100]\tSamples: [58500/75000]\tTrain Loss: 1132.1723802083334\tTime: 0:00:00.232805\n",
            "Epoch: [79/100]\tSamples: [59250/75000]\tTrain Loss: 1126.24478125\tTime: 0:00:00.229750\n",
            "Epoch: [80/100]\tSamples: [60000/75000]\tTrain Loss: 1119.92203125\tTime: 0:00:00.220606\n",
            "Epoch: [81/100]\tSamples: [60750/75000]\tTrain Loss: 1128.4604583333332\tTime: 0:00:00.212466\n",
            "Epoch: [82/100]\tSamples: [61500/75000]\tTrain Loss: 1134.27275\tTime: 0:00:00.217706\n",
            "Epoch: [83/100]\tSamples: [62250/75000]\tTrain Loss: 1124.7082135416667\tTime: 0:00:00.237354\n",
            "Epoch: [84/100]\tSamples: [63000/75000]\tTrain Loss: 1127.1902552083334\tTime: 0:00:00.215727\n",
            "Epoch: [85/100]\tSamples: [63750/75000]\tTrain Loss: 1125.352828125\tTime: 0:00:00.227106\n",
            "Epoch: [86/100]\tSamples: [64500/75000]\tTrain Loss: 1122.1413020833334\tTime: 0:00:00.216226\n",
            "Epoch: [87/100]\tSamples: [65250/75000]\tTrain Loss: 1125.9175\tTime: 0:00:00.233006\n",
            "Epoch: [88/100]\tSamples: [66000/75000]\tTrain Loss: 1127.0008541666666\tTime: 0:00:00.212072\n",
            "Epoch: [89/100]\tSamples: [66750/75000]\tTrain Loss: 1127.2934479166668\tTime: 0:00:00.221718\n",
            "Epoch: [90/100]\tSamples: [67500/75000]\tTrain Loss: 1124.5438541666667\tTime: 0:00:00.214550\n",
            "Epoch: [91/100]\tSamples: [68250/75000]\tTrain Loss: 1119.8561770833333\tTime: 0:00:00.225750\n",
            "Epoch: [92/100]\tSamples: [69000/75000]\tTrain Loss: 1129.5133125\tTime: 0:00:00.214854\n",
            "Epoch: [93/100]\tSamples: [69750/75000]\tTrain Loss: 1122.1587239583334\tTime: 0:00:00.228464\n",
            "Epoch: [94/100]\tSamples: [70500/75000]\tTrain Loss: 1124.2488125\tTime: 0:00:00.219024\n",
            "Epoch: [95/100]\tSamples: [71250/75000]\tTrain Loss: 1129.4233697916666\tTime: 0:00:00.213226\n",
            "Epoch: [96/100]\tSamples: [72000/75000]\tTrain Loss: 1126.408625\tTime: 0:00:00.219230\n",
            "Epoch: [97/100]\tSamples: [72750/75000]\tTrain Loss: 1125.444046875\tTime: 0:00:00.231518\n",
            "Epoch: [98/100]\tSamples: [73500/75000]\tTrain Loss: 1126.4723333333334\tTime: 0:00:00.216827\n",
            "Epoch: [99/100]\tSamples: [74250/75000]\tTrain Loss: 1124.087015625\tTime: 0:00:00.225471\n",
            "Epoch: [100/100]\tSamples: [75000/75000]\tTrain Loss: 1124.9384375\tTime: 0:00:00.214845\n"
          ]
        }
      ],
      "source": [
        "avitm = train_avitm(documents_train_0, input_size_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4kezlbmRaRV"
      },
      "source": [
        "### Document-topic distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PXTjce2LlVRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa62327-56b5-41ff-ca3c-8cb886239ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  7.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 0\n",
            "(750, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic = get_doc_topic_distribution(avitm, documents_train_0, n_samples=5) # get all the topic predictions\n",
        "print(\"Document-topic distribution node 0\")\n",
        "print(np.array(doc_topic).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEE_kyedRgYv"
      },
      "source": [
        "### Word-topic distributions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-ycAfAPI8k4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f485c12-d248-4604-f2cf-d51dc5d071f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00059334 0.         0.         ... 0.         0.00065115 0.        ]\n",
            " [0.00063145 0.         0.         ... 0.         0.00063369 0.        ]\n",
            " [0.00059186 0.         0.         ... 0.         0.00063848 0.        ]\n",
            " ...\n",
            " [0.00083595 0.         0.         ... 0.         0.00049691 0.        ]\n",
            " [0.00056034 0.         0.         ... 0.         0.00064591 0.        ]\n",
            " [0.00055954 0.         0.         ... 0.         0.00065185 0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000000567233656"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "word_topic = convert_topic_word_to_init_size(vocab_size, avitm, n_topics, id2token_0, all_words)\n",
        "word_topic.shape\n",
        "print(word_topic)\n",
        "sum(word_topic[0,:])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_node0, pred_thetas = eval_avitm(documents_val_0, avitm, 50)\n",
        "print(preds_node0)"
      ],
      "metadata": {
        "id": "6g05SRU3-ItR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da40d195-6250-4da1-e27d-342cb3586d91"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [20/20]: : 20it [00:01, 10.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 732, 1054, 1323,  ..., 1121,  677, 1607],\n",
            "        [1192,  360,  614,  ..., 1611,  321, 1431],\n",
            "        [ 914,  142,  515,  ...,  233, 1467,  803],\n",
            "        ...,\n",
            "        [ 273,  271,  542,  ...,  325, 1100,   43],\n",
            "        [ 732, 1054,  854,  ...,  383,  978,  867],\n",
            "        [1535,  519, 1425,  ...,  508,  974, 1143]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usiERbR-8Roe"
      },
      "source": [
        "# 4. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Ng1ykc1A9wkn"
      },
      "outputs": [],
      "source": [
        "doc_topic_centr_node_0_train = doc_topic_centr[0:cut,:]\n",
        "doc_topic_centr_node_0_val = doc_topic_centr[cut:n_docs,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lceg9pT2zeP"
      },
      "source": [
        "### Doc-topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VF0WXTEy2yu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1edf08-d19a-4e47-d570-6347c5ab09de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference in evaluation (train) of doc similarity (node 0): 513.9165318187355\n",
            "Difference in evaluation (val) of doc similarity (node 0): 132.7143677569648\n",
            "Difference in evaluation (train) of doc similarity (centr): 473.8053300834349\n",
            "Difference in evaluation (val) of doc similarity (centr): 158.36713751694188\n"
          ]
        }
      ],
      "source": [
        "# doc_topics_all[0]\n",
        "sim_mat_theoretical_train = np.sqrt(doc_topics_train_0).dot(np.sqrt(doc_topics_train_0.T))\n",
        "sim_mat_actual_train = np.sqrt(doc_topic).dot(np.sqrt(doc_topic.T))\n",
        "print('Difference in evaluation (train) of doc similarity (node 0):', np.sum(np.abs(sim_mat_theoretical_train - sim_mat_actual_train))/len(doc_topics_train_0))\n",
        "\n",
        "sim_mat_theoretical_val = np.sqrt(doc_topics_val_0).dot(np.sqrt(doc_topics_val_0.T))\n",
        "sim_mat_actual_val = np.sqrt(pred_thetas).dot(np.sqrt(pred_thetas.T))\n",
        "print('Difference in evaluation (val) of doc similarity (node 0):', np.sum(np.abs(sim_mat_theoretical_val - sim_mat_actual_val))/len(doc_topics_val_0))\n",
        "\n",
        "sim_mat_actual_centr_train = np.sqrt(doc_topic_centr_node_0_train).dot(np.sqrt(doc_topic_centr_node_0_train.T))\n",
        "print('Difference in evaluation (train) of doc similarity (centr):', np.sum(np.abs(sim_mat_theoretical_train - sim_mat_actual_centr_train))/len(doc_topics_train_0))\n",
        "\n",
        "sim_mat_actual_centr_val= np.sqrt(doc_topic_centr_node_0_val).dot(np.sqrt(doc_topic_centr_node_0_val.T))\n",
        "print('Difference in evaluation (val) of doc similarity (centr):', np.sum(np.abs(sim_mat_theoretical_val - sim_mat_actual_centr_val))/len(doc_topics_val_0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7WwmitW3VAz"
      },
      "source": [
        "### Topic-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Yr9sMP2X3b8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a713d1cc-bb4f-434e-ef00-28404a79e7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tópicos (equivalentes) evaluados correctamente (node 0): 5.184709469167052\n",
            "Tópicos (equivalentes) evaluados correctamente (centr): 6.691780624053224\n"
          ]
        }
      ],
      "source": [
        "print('Tópicos (equivalentes) evaluados correctamente (node 0):', np.sum(np.max(np.sqrt(word_topic).dot(np.sqrt(topic_vectors.T)), axis=0)))\n",
        "print('Tópicos (equivalentes) evaluados correctamente (centr):', np.sum(np.max(np.sqrt(word_topic_centr).dot(np.sqrt(topic_vectors.T)), axis=0)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Centralized_ProdLDA_with_eval.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}