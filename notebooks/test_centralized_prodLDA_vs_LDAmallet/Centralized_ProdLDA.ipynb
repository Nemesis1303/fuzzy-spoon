{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sib1HSks6Xqv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.special import softmax\n",
        "import multiprocessing as mp\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CObHMSd6LLz"
      },
      "source": [
        "# Installing ProdLDA\n",
        "**Restart notbook after the installation!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VDAzxJA6FK5",
        "outputId": "b5d5f41f-df7d-4fd3-ae6d-60738d5a71e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PyTorchAVITM'...\n",
            "remote: Enumerating objects: 19052, done.\u001b[K\n",
            "remote: Total 19052 (delta 0), reused 0 (delta 0), pack-reused 19052\u001b[K\n",
            "Receiving objects: 100% (19052/19052), 132.62 MiB | 22.94 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "Checking out files: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/estebandito22/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6kW5jO66UKj"
      },
      "source": [
        "# 1. Creation of synthetic corpus\n",
        "\n",
        "We consider a scenario with n parties, each of them as an associated corpus.\n",
        "To generate the corpus associated with each of the parties, we consider a common beta distribution (word-topic distribution), but we freeze different topics/ assign different asymmetric Dirichlet priors favoring different topics at the time of generating the document that composes each party's corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twSZ3G0p6d1z"
      },
      "source": [
        "## 1.1. Function for permuting the Dirichlet prior at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "tXdkpdrh6Thn"
      },
      "outputs": [],
      "source": [
        "def rotateArray(arr, n, d):\n",
        "    temp = []\n",
        "    i = 0\n",
        "    while (i < d):\n",
        "        temp.append(arr[i])\n",
        "        i = i + 1\n",
        "    i = 0\n",
        "    while (d < n):\n",
        "        arr[i] = arr[d]\n",
        "        i = i + 1\n",
        "        d = d + 1\n",
        "    arr[:] = arr[: i] + temp\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyFA9eGH6hGH"
      },
      "source": [
        "## 1.2. Topic modeling and node settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5DmfSiuR6iI0"
      },
      "outputs": [],
      "source": [
        "# Topic modeling settings\n",
        "vocab_size = 5000\n",
        "n_topics = 50\n",
        "beta = 1e-2\n",
        "alpha = 1/n_topics\n",
        "n_docs = 1000\n",
        "nwords = (150, 250) #Min and max lengths of the documents\n",
        "\n",
        "# Nodes settings\n",
        "n_nodes = 5\n",
        "frozen_topics = 5\n",
        "prior_frozen = frozen_topics * [alpha]\n",
        "own_topics = int((n_topics-frozen_topics)/n_nodes)\n",
        "prior_nofrozen = own_topics * [alpha] + (n_topics-frozen_topics-own_topics) * [alpha/10000]\n",
        "#print(prior_frozen + prior_nofrozen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ylo9Vsu6zpX"
      },
      "source": [
        "## 1.3. Topics generation (common for all nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy3AuOSx6qc1",
        "outputId": "90f123c1-2e7a-4128-8407-fcd7f79f658d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades ordenadas para el primer vector de tópicos:\n",
            "[0.0587044  0.04676506 0.04078716 ... 0.         0.         0.        ]\n",
            "(50, 5000)\n"
          ]
        }
      ],
      "source": [
        "topic_vectors = np.random.dirichlet(vocab_size*[beta], n_topics)\n",
        "print('Probabilidades ordenadas para el primer vector de tópicos:')\n",
        "print(np.sort(topic_vectors[0])[::-1])\n",
        "print(topic_vectors.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1W6ldfZeJsT",
        "outputId": "74684924-55c3-48f8-e144-a215396923a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tópicos (equivalentes) identificados correctamente (true): 50.00000000000005\n",
            "Tópicos (equivalentes) identificados correctamente (random): 3.7104788499983465\n"
          ]
        }
      ],
      "source": [
        "#Here we compare alignment of the topic_vector matrix with itself and with another randomly generated matrix\n",
        "print('Tópicos (equivalentes) identificados correctamente (true):', np.sum(np.max(np.sqrt(topic_vectors).dot(np.sqrt(topic_vectors.T)), axis=0)))\n",
        "topic_vectors2 = np.random.dirichlet(vocab_size*[beta], n_topics)\n",
        "print('Tópicos (equivalentes) identificados correctamente (random):', np.sum(np.max(np.sqrt(topic_vectors2).dot(np.sqrt(topic_vectors.T)), axis=0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaQEe-yD6vl_"
      },
      "source": [
        "## 1.4. Generation of document topic proportions and documents for each node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VZJEmEhheNva"
      },
      "outputs": [],
      "source": [
        "# Step 2 - generation of document topic proportions\n",
        "doc_topics_all = []\n",
        "for i in np.arange(n_nodes):\n",
        "    doc_topics = np.random.dirichlet(prior_frozen + prior_nofrozen, n_docs)\n",
        "    prior_nofrozen = rotateArray(prior_nofrozen, len(prior_nofrozen), own_topics)\n",
        "    doc_topics_all.append(doc_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "N-BziCW56vFL"
      },
      "outputs": [],
      "source": [
        "# Step 3 - Document generation\n",
        "documents_all = []\n",
        "z_all = []\n",
        "\n",
        "for i in np.arange(n_nodes):\n",
        "    documents = [] # Document words\n",
        "    #z = [] # Assignments\n",
        "    for docid in np.arange(n_docs):\n",
        "        doc_len = np.random.randint(low=nwords[0], high=nwords[1])\n",
        "        this_doc_words = []\n",
        "        #this_doc_assigns = []\n",
        "        for wd_idx in np.arange(doc_len):\n",
        "            tpc = np.nonzero(np.random.multinomial(1, doc_topics_all[i][docid]))[0][0]\n",
        "            #this_doc_assigns.append(tpc)\n",
        "            word = np.nonzero(np.random.multinomial(1, topic_vectors[tpc]))[0][0]\n",
        "            this_doc_words.append('wd'+str(word))\n",
        "        #z.append(this_doc_assigns)\n",
        "        documents.append(this_doc_words)\n",
        "    documents_all.append(documents)\n",
        "    #z_all.append(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlZysxpSllMB",
        "outputId": "8059874a-0971-49a4-f4e6-0f7361a2f7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:719: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  val = np.asanyarray(val)\n"
          ]
        }
      ],
      "source": [
        "np.savez('synthetic_10000_beta_1.npz', n_nodes = n_nodes, vocab_size=vocab_size, n_topics=n_topics, frozen_topics = frozen_topics, beta=beta, alpha=alpha,\n",
        "        n_docs=n_docs, nwords=nwords, topic_vectors=topic_vectors, doc_topics=doc_topics_all,\n",
        "        documents=documents_all, z=z_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GatNJky5hQSr"
      },
      "outputs": [],
      "source": [
        "doc_topics_all_gt = doc_topics_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuJUlOIJ69iw"
      },
      "source": [
        "# 2. ProdLDA model at node 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14tnh0ndFpb0",
        "outputId": "7b560e93-4cbd-403e-9701-91dd9c84e847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM/pytorchavitm/datasets\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM/pytorchavitm/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "icRzu2ZE3naH"
      },
      "outputs": [],
      "source": [
        "from bow import BOWDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4okSYycQaOK",
        "outputId": "20ee79a1-cd0c-4240-c8ab-3bdceca269e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "gFPzE3yN3pLI"
      },
      "outputs": [],
      "source": [
        "from pytorchavitm import AVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ZGNArrP50AhG"
      },
      "outputs": [],
      "source": [
        "def train_avitm(docs_train):\n",
        "  \"\"\"Trains an AVITM model with ProdLDA.\"\"\"   \n",
        "\n",
        "  cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                      max_df=0.99, min_df=0.01, binary=False)\n",
        "  \n",
        "  docs = [\" \".join(docs_train[i]) for i in np.arange(len(docs_train))]\n",
        "\n",
        "  train_bow = cv.fit_transform(docs)\n",
        "  train_bow = train_bow.toarray()\n",
        "\n",
        "  idx2token = cv.get_feature_names()\n",
        "  input_size = len(idx2token)\n",
        "\n",
        "  id2token = {k: v for k, v in zip(range(0, len(idx2token)), idx2token)}\n",
        "\n",
        "  train_data = BOWDataset(train_bow, idx2token)\n",
        "\n",
        "  avitm = AVITM(input_size=input_size, n_components=n_topics, model_type='prodLDA',\n",
        "                hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "                learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "                solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
        "  \n",
        "  avitm.fit(train_data)\n",
        "\n",
        "  return train_data, avitm, id2token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "orpK--hqxQkG"
      },
      "outputs": [],
      "source": [
        "def get_doc_topic_distribution(avitm, dataset, n_samples=20):\n",
        "    \"\"\"Given a trained AVITM model, it gets its associated document-topic distribution.\n",
        "\n",
        "    Args:\n",
        "        * n_samples (int, optional): Defaults to 20.\n",
        "\n",
        "    Returns:\n",
        "        * ndarray : Document-topics distribution\n",
        "    \"\"\"     \n",
        "    avitm.model.eval()\n",
        "\n",
        "    loader = DataLoader(\n",
        "            avitm.train_data, batch_size=avitm.batch_size, shuffle=True,\n",
        "            num_workers=mp.cpu_count())\n",
        "\n",
        "    pbar = tqdm(n_samples, position=0, leave=True)\n",
        "\n",
        "    final_thetas = []\n",
        "    for sample_index in range(n_samples):\n",
        "        with torch.no_grad():\n",
        "            collect_theta = []\n",
        "\n",
        "            for batch_samples in loader:\n",
        "                X = batch_samples['X']\n",
        "\n",
        "                if avitm.USE_CUDA:\n",
        "                  X = X.cuda()\n",
        "\n",
        "                # forward pass\n",
        "                avitm.model.zero_grad()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                  posterior_mu, posterior_log_sigma = avitm.model.inf_net(X)\n",
        "\n",
        "                  # Generate samples from theta\n",
        "                  theta = F.softmax(\n",
        "                          avitm.model.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
        "                  theta = avitm.model.drop_theta(theta)\n",
        "\n",
        "                collect_theta.extend(theta.cpu().numpy().tolist())\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
        "\n",
        "            final_thetas.append(np.array(collect_theta))\n",
        "    pbar.close()\n",
        "    return np.sum(final_thetas, axis=0) / n_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "douV1llyTM4b"
      },
      "outputs": [],
      "source": [
        "def get_topic_word_distribution(avtim_model):\n",
        "  \"\"\"Given a trained AVITM model, it gets its associated topic-word distribution.\n",
        "\n",
        "    Args:\n",
        "        * avtim_model (AVITM): Trained AVITM model.\n",
        "\n",
        "    Returns:\n",
        "        * ndarray : topic-word distribution\n",
        "    \"\"\"     \n",
        "  topic_word_matrix = avtim_model.model.beta.cpu().detach().numpy()\n",
        "  wd = softmax(topic_word_matrix, axis=1)\n",
        "  return normalize(wd,axis=1,norm='l1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "CT-rniqA78QO"
      },
      "outputs": [],
      "source": [
        "def convert_topic_word_to_init_size(vocab_size, model, ntopics,\n",
        "                                    id2token, all_words):\n",
        "    \"\"\"It converts the topic-word distribution matrix obtained from the\n",
        "    training of a model into a matrix with the dimensions of the original\n",
        "    topic-word distribution, assigning zeros to those words that are not\n",
        "    present in the corpus. \n",
        "    It is only of use in case we are training a model over a synthetic dataset,\n",
        "    so as to later compare the performance of the attained model in what regards\n",
        "    to the similarity between the original and the trained model.\n",
        "\n",
        "    Args:\n",
        "        * vocab_size (int):       Size of the synethic'data vocabulary.\n",
        "        * model (AVITM):          Model whose topic-word matrix is being transformed.\n",
        "        * ntopics (int):          Number of topics of the trained model.\n",
        "        * id2token (List[tuple]): Mappings with the content of the document-term matrix.\n",
        "        * all_words (List[str]):  List of all the words of the vocabulary of size vocab_size.\n",
        "\n",
        "    Returns:\n",
        "        * ndarray: Normalized transormed topic-word distribution.\n",
        "    \"\"\"\n",
        "    w_t_distrib = np.zeros((ntopics, vocab_size), dtype=np.float64)\n",
        "    wd = get_topic_word_distribution(model)\n",
        "    for i in np.arange(ntopics):\n",
        "        for idx, word in id2token.items():\n",
        "            for j in np.arange(len(all_words)):\n",
        "                if all_words[j] == word:\n",
        "                    w_t_distrib[i,j] = wd[i][idx]\n",
        "                    break\n",
        "    #normalized_array = normalize(w_t_distrib,axis=1,norm='l1')\n",
        "    return w_t_distrib\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "wchhyQ5bDIhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc609f7d-485d-4e87-ad3d-bfb91c2db11e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 50\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.98\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 1542.7358515625\tTime: 0:00:00.498696\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 1461.7211484375\tTime: 0:00:00.453321\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 1387.38180859375\tTime: 0:00:00.430096\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 1336.92373828125\tTime: 0:00:00.442578\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 1301.72071875\tTime: 0:00:00.430458\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 1277.29698828125\tTime: 0:00:00.417067\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 1257.553171875\tTime: 0:00:00.383762\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 1247.9364375\tTime: 0:00:00.364154\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 1233.490140625\tTime: 0:00:00.401820\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 1220.6002890625\tTime: 0:00:00.424178\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 1218.7162421875\tTime: 0:00:00.445065\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 1207.23956640625\tTime: 0:00:00.424911\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 1200.32479296875\tTime: 0:00:00.419237\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 1196.4441640625\tTime: 0:00:00.424543\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 1194.7806328125\tTime: 0:00:00.454607\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 1193.9832421875\tTime: 0:00:00.397285\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 1183.0585625\tTime: 0:00:00.373997\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 1186.3645703125\tTime: 0:00:00.358591\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 1179.39825390625\tTime: 0:00:00.333370\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 1177.89161328125\tTime: 0:00:00.433596\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 1178.64280078125\tTime: 0:00:00.448175\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 1181.17919921875\tTime: 0:00:00.424180\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 1174.97503125\tTime: 0:00:00.418524\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 1175.84721875\tTime: 0:00:00.437062\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 1171.25499609375\tTime: 0:00:00.422529\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 1170.71075\tTime: 0:00:00.417250\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 1171.53840625\tTime: 0:00:00.425239\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 1169.3628984375\tTime: 0:00:00.413904\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 1159.3809609375\tTime: 0:00:00.421306\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 1158.82166015625\tTime: 0:00:00.406762\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 1163.41080859375\tTime: 0:00:00.392021\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 1163.94681640625\tTime: 0:00:00.369541\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 1169.6568515625\tTime: 0:00:00.329272\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 1165.59978515625\tTime: 0:00:00.349739\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 1155.050953125\tTime: 0:00:00.428170\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 1157.60356640625\tTime: 0:00:00.394888\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 1152.8385234375\tTime: 0:00:00.418827\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 1158.782234375\tTime: 0:00:00.414177\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 1157.94312890625\tTime: 0:00:00.409759\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 1155.6713125\tTime: 0:00:00.430967\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 1156.5259296875\tTime: 0:00:00.438492\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 1155.87880078125\tTime: 0:00:00.392020\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 1158.9964375\tTime: 0:00:00.343789\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 1152.64089453125\tTime: 0:00:00.357758\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 1148.98047265625\tTime: 0:00:00.367909\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 1153.18733203125\tTime: 0:00:00.340178\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 1151.05955859375\tTime: 0:00:00.357099\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 1148.40968359375\tTime: 0:00:00.424478\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 1153.679640625\tTime: 0:00:00.425374\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 1151.4257734375\tTime: 0:00:00.459602\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 1153.300046875\tTime: 0:00:00.427377\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 1145.45280078125\tTime: 0:00:00.428516\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 1150.13\tTime: 0:00:00.418762\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 1148.56603515625\tTime: 0:00:00.405490\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 1151.58534375\tTime: 0:00:00.420478\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 1149.75436328125\tTime: 0:00:00.388914\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 1151.55235546875\tTime: 0:00:00.415944\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 1148.62958203125\tTime: 0:00:00.436580\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 1145.966640625\tTime: 0:00:00.421411\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 1150.14788671875\tTime: 0:00:00.435963\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 1143.495\tTime: 0:00:00.411796\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 1153.0712734375\tTime: 0:00:00.389558\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 1146.48784375\tTime: 0:00:00.348751\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 1148.842109375\tTime: 0:00:00.344180\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 1151.91947265625\tTime: 0:00:00.344669\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 1148.71480859375\tTime: 0:00:00.346881\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 1144.154\tTime: 0:00:00.370889\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 1147.84332421875\tTime: 0:00:00.350538\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 1145.451\tTime: 0:00:00.342430\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 1145.07485546875\tTime: 0:00:00.352868\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 1148.234859375\tTime: 0:00:00.348794\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 1147.00501953125\tTime: 0:00:00.352446\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 1152.18823828125\tTime: 0:00:00.342687\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 1144.28421875\tTime: 0:00:00.348513\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 1144.619953125\tTime: 0:00:00.339273\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 1149.27904296875\tTime: 0:00:00.341256\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 1145.5293515625\tTime: 0:00:00.362616\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 1144.7129765625\tTime: 0:00:00.356888\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 1149.88774609375\tTime: 0:00:00.342487\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 1139.008078125\tTime: 0:00:00.336289\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 1147.402828125\tTime: 0:00:00.354241\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 1142.88497265625\tTime: 0:00:00.345144\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 1146.31540625\tTime: 0:00:00.347781\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 1143.23453515625\tTime: 0:00:00.342749\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 1146.40297265625\tTime: 0:00:00.381232\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 1148.1968046875\tTime: 0:00:00.347086\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 1144.6076015625\tTime: 0:00:00.336384\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 1139.29565234375\tTime: 0:00:00.343792\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 1141.521109375\tTime: 0:00:00.341624\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 1140.42628515625\tTime: 0:00:00.350222\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 1142.63229296875\tTime: 0:00:00.341349\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 1141.91142578125\tTime: 0:00:00.340487\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 1140.9940859375\tTime: 0:00:00.352470\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 1142.82612890625\tTime: 0:00:00.352685\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 1140.0366796875\tTime: 0:00:00.413996\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 1144.6553515625\tTime: 0:00:00.462448\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 1146.03431640625\tTime: 0:00:00.417093\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 1141.43971484375\tTime: 0:00:00.429606\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 1141.93278515625\tTime: 0:00:00.421981\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 1142.43548828125\tTime: 0:00:00.437413\n"
          ]
        }
      ],
      "source": [
        "corpus_node = documents_all[0]\n",
        "train_data, avitm, id2token = train_avitm(corpus_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4kezlbmRaRV"
      },
      "source": [
        "### Document-topic distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXTjce2LlVRG",
        "outputId": "e89b26f9-1470-4510-bf8d-365c4c8a1a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:01,  4.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 0\n",
            "(1000, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic = get_doc_topic_distribution(avitm, train_data, n_samples=5) # get all the topic predictions\n",
        "print(\"Document-topic distribution node 0\")\n",
        "print(np.array(doc_topic).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEE_kyedRgYv"
      },
      "source": [
        "### Word-topic distributions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "BdM2jxRJUvk3"
      },
      "outputs": [],
      "source": [
        "all_words = ['wd'+str(word) for word in np.arange(vocab_size+1) if word > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "-ycAfAPI8k4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9e2aea-e873-42be-b78b-e7a16121e82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0005489  0.         0.         ... 0.00069365 0.         0.        ]\n",
            " [0.0005847  0.         0.         ... 0.00059868 0.         0.        ]\n",
            " [0.00052104 0.         0.         ... 0.00066779 0.         0.        ]\n",
            " ...\n",
            " [0.00091556 0.         0.         ... 0.00063412 0.         0.        ]\n",
            " [0.00103318 0.         0.         ... 0.00060432 0.         0.        ]\n",
            " [0.00056694 0.         0.         ... 0.00065327 0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999861756805"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "word_topic = convert_topic_word_to_init_size(vocab_size, avitm, n_topics, id2token, all_words)\n",
        "word_topic.shape\n",
        "print(word_topic)\n",
        "sum(word_topic[0,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QieP5DdU7zY0"
      },
      "source": [
        "# 3. Centralized ProdLDA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTQrxfRpRrD1",
        "outputId": "3cbc23c8-7d9a-4246-b84c-0c584b7a963f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "documents_centr = [*documents_all[0], *documents_all[1], *documents_all[2], *documents_all[3], *documents_all[4]]\n",
        "len(documents_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kxW_tMtFRyfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40c51d6-502b-4ac0-a760-e94f0594a60c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 50\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.98\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100]\tSamples: [5000/500000]\tTrain Loss: 1503.049188671875\tTime: 0:00:01.596424\n",
            "Epoch: [2/100]\tSamples: [10000/500000]\tTrain Loss: 1364.6215447265624\tTime: 0:00:01.347067\n",
            "Epoch: [3/100]\tSamples: [15000/500000]\tTrain Loss: 1309.71843125\tTime: 0:00:01.348188\n",
            "Epoch: [4/100]\tSamples: [20000/500000]\tTrain Loss: 1275.6063173828124\tTime: 0:00:01.477696\n",
            "Epoch: [5/100]\tSamples: [25000/500000]\tTrain Loss: 1251.924334765625\tTime: 0:00:01.429612\n",
            "Epoch: [6/100]\tSamples: [30000/500000]\tTrain Loss: 1230.585549609375\tTime: 0:00:01.342450\n",
            "Epoch: [7/100]\tSamples: [35000/500000]\tTrain Loss: 1219.229098828125\tTime: 0:00:01.386390\n",
            "Epoch: [8/100]\tSamples: [40000/500000]\tTrain Loss: 1208.558864453125\tTime: 0:00:01.614898\n",
            "Epoch: [9/100]\tSamples: [45000/500000]\tTrain Loss: 1202.369927734375\tTime: 0:00:01.508136\n",
            "Epoch: [10/100]\tSamples: [50000/500000]\tTrain Loss: 1197.0090634765625\tTime: 0:00:01.307159\n",
            "Epoch: [11/100]\tSamples: [55000/500000]\tTrain Loss: 1191.0727953125\tTime: 0:00:01.475113\n",
            "Epoch: [12/100]\tSamples: [60000/500000]\tTrain Loss: 1188.39196640625\tTime: 0:00:01.430060\n",
            "Epoch: [13/100]\tSamples: [65000/500000]\tTrain Loss: 1187.1243431640626\tTime: 0:00:01.461840\n",
            "Epoch: [14/100]\tSamples: [70000/500000]\tTrain Loss: 1184.7820822265626\tTime: 0:00:01.419640\n",
            "Epoch: [15/100]\tSamples: [75000/500000]\tTrain Loss: 1184.3988513671875\tTime: 0:00:01.468150\n",
            "Epoch: [16/100]\tSamples: [80000/500000]\tTrain Loss: 1183.0387892578126\tTime: 0:00:01.314897\n",
            "Epoch: [17/100]\tSamples: [85000/500000]\tTrain Loss: 1182.7142556640624\tTime: 0:00:01.315591\n",
            "Epoch: [18/100]\tSamples: [90000/500000]\tTrain Loss: 1182.9395720703126\tTime: 0:00:01.323228\n",
            "Epoch: [19/100]\tSamples: [95000/500000]\tTrain Loss: 1178.8810033203124\tTime: 0:00:01.458071\n",
            "Epoch: [20/100]\tSamples: [100000/500000]\tTrain Loss: 1178.0827603515625\tTime: 0:00:01.350664\n",
            "Epoch: [21/100]\tSamples: [105000/500000]\tTrain Loss: 1177.8284171875\tTime: 0:00:01.336611\n",
            "Epoch: [22/100]\tSamples: [110000/500000]\tTrain Loss: 1176.6065033203124\tTime: 0:00:01.481510\n",
            "Epoch: [23/100]\tSamples: [115000/500000]\tTrain Loss: 1177.5574892578124\tTime: 0:00:01.534301\n",
            "Epoch: [24/100]\tSamples: [120000/500000]\tTrain Loss: 1178.596778125\tTime: 0:00:01.439287\n",
            "Epoch: [25/100]\tSamples: [125000/500000]\tTrain Loss: 1172.9532892578125\tTime: 0:00:01.437807\n",
            "Epoch: [26/100]\tSamples: [130000/500000]\tTrain Loss: 1175.568908203125\tTime: 0:00:01.430313\n",
            "Epoch: [27/100]\tSamples: [135000/500000]\tTrain Loss: 1173.4627494140625\tTime: 0:00:01.381300\n",
            "Epoch: [28/100]\tSamples: [140000/500000]\tTrain Loss: 1172.8331806640624\tTime: 0:00:01.453805\n",
            "Epoch: [29/100]\tSamples: [145000/500000]\tTrain Loss: 1174.5081298828125\tTime: 0:00:01.326782\n",
            "Epoch: [30/100]\tSamples: [150000/500000]\tTrain Loss: 1171.398301953125\tTime: 0:00:01.358703\n",
            "Epoch: [31/100]\tSamples: [155000/500000]\tTrain Loss: 1174.6850857421875\tTime: 0:00:01.337505\n",
            "Epoch: [32/100]\tSamples: [160000/500000]\tTrain Loss: 1170.3679994140625\tTime: 0:00:01.341178\n",
            "Epoch: [33/100]\tSamples: [165000/500000]\tTrain Loss: 1168.1728193359374\tTime: 0:00:01.370721\n",
            "Epoch: [34/100]\tSamples: [170000/500000]\tTrain Loss: 1169.0591630859376\tTime: 0:00:01.601061\n",
            "Epoch: [35/100]\tSamples: [175000/500000]\tTrain Loss: 1171.21751875\tTime: 0:00:01.376010\n",
            "Epoch: [36/100]\tSamples: [180000/500000]\tTrain Loss: 1167.784109765625\tTime: 0:00:01.361102\n",
            "Epoch: [37/100]\tSamples: [185000/500000]\tTrain Loss: 1167.8120388671875\tTime: 0:00:01.526531\n",
            "Epoch: [38/100]\tSamples: [190000/500000]\tTrain Loss: 1167.7198953125\tTime: 0:00:01.346086\n",
            "Epoch: [39/100]\tSamples: [195000/500000]\tTrain Loss: 1168.758244140625\tTime: 0:00:01.419366\n",
            "Epoch: [40/100]\tSamples: [200000/500000]\tTrain Loss: 1168.4512619140626\tTime: 0:00:01.385758\n",
            "Epoch: [41/100]\tSamples: [205000/500000]\tTrain Loss: 1173.3988375\tTime: 0:00:01.602014\n",
            "Epoch: [42/100]\tSamples: [210000/500000]\tTrain Loss: 1167.942941015625\tTime: 0:00:01.432861\n",
            "Epoch: [43/100]\tSamples: [215000/500000]\tTrain Loss: 1166.5389015625\tTime: 0:00:01.388341\n",
            "Epoch: [44/100]\tSamples: [220000/500000]\tTrain Loss: 1165.455033203125\tTime: 0:00:01.450329\n",
            "Epoch: [45/100]\tSamples: [225000/500000]\tTrain Loss: 1167.9172556640624\tTime: 0:00:01.533232\n",
            "Epoch: [46/100]\tSamples: [230000/500000]\tTrain Loss: 1164.856869140625\tTime: 0:00:01.382349\n",
            "Epoch: [47/100]\tSamples: [235000/500000]\tTrain Loss: 1166.8923943359375\tTime: 0:00:01.353360\n",
            "Epoch: [48/100]\tSamples: [240000/500000]\tTrain Loss: 1163.9731517578125\tTime: 0:00:01.536887\n",
            "Epoch: [49/100]\tSamples: [245000/500000]\tTrain Loss: 1169.0860515625\tTime: 0:00:01.484060\n",
            "Epoch: [50/100]\tSamples: [250000/500000]\tTrain Loss: 1168.3643138671875\tTime: 0:00:01.557232\n",
            "Epoch: [51/100]\tSamples: [255000/500000]\tTrain Loss: 1169.718001953125\tTime: 0:00:01.590866\n",
            "Epoch: [52/100]\tSamples: [260000/500000]\tTrain Loss: 1164.07262109375\tTime: 0:00:01.384606\n",
            "Epoch: [53/100]\tSamples: [265000/500000]\tTrain Loss: 1166.5197056640625\tTime: 0:00:01.318593\n",
            "Epoch: [54/100]\tSamples: [270000/500000]\tTrain Loss: 1166.7276392578126\tTime: 0:00:01.504293\n",
            "Epoch: [55/100]\tSamples: [275000/500000]\tTrain Loss: 1168.329148828125\tTime: 0:00:01.382277\n",
            "Epoch: [56/100]\tSamples: [280000/500000]\tTrain Loss: 1164.980394140625\tTime: 0:00:01.414077\n",
            "Epoch: [57/100]\tSamples: [285000/500000]\tTrain Loss: 1162.1463623046875\tTime: 0:00:01.412841\n",
            "Epoch: [58/100]\tSamples: [290000/500000]\tTrain Loss: 1164.7473751953125\tTime: 0:00:01.430270\n",
            "Epoch: [59/100]\tSamples: [295000/500000]\tTrain Loss: 1165.272884375\tTime: 0:00:01.601773\n",
            "Epoch: [60/100]\tSamples: [300000/500000]\tTrain Loss: 1161.974678515625\tTime: 0:00:01.384765\n",
            "Epoch: [61/100]\tSamples: [305000/500000]\tTrain Loss: 1163.4004203125\tTime: 0:00:01.366467\n",
            "Epoch: [62/100]\tSamples: [310000/500000]\tTrain Loss: 1165.62478984375\tTime: 0:00:01.482748\n",
            "Epoch: [63/100]\tSamples: [315000/500000]\tTrain Loss: 1166.4747447265624\tTime: 0:00:01.352623\n",
            "Epoch: [64/100]\tSamples: [320000/500000]\tTrain Loss: 1164.09750859375\tTime: 0:00:01.372419\n",
            "Epoch: [65/100]\tSamples: [325000/500000]\tTrain Loss: 1162.732485546875\tTime: 0:00:01.355231\n",
            "Epoch: [66/100]\tSamples: [330000/500000]\tTrain Loss: 1165.3609580078125\tTime: 0:00:01.415944\n",
            "Epoch: [67/100]\tSamples: [335000/500000]\tTrain Loss: 1161.9155818359375\tTime: 0:00:01.546669\n",
            "Epoch: [68/100]\tSamples: [340000/500000]\tTrain Loss: 1159.850716796875\tTime: 0:00:01.400985\n",
            "Epoch: [69/100]\tSamples: [345000/500000]\tTrain Loss: 1162.674733203125\tTime: 0:00:01.428217\n",
            "Epoch: [70/100]\tSamples: [350000/500000]\tTrain Loss: 1161.4827486328124\tTime: 0:00:01.426419\n",
            "Epoch: [71/100]\tSamples: [355000/500000]\tTrain Loss: 1159.773255078125\tTime: 0:00:01.493028\n",
            "Epoch: [72/100]\tSamples: [360000/500000]\tTrain Loss: 1166.7348123046875\tTime: 0:00:01.540400\n",
            "Epoch: [73/100]\tSamples: [365000/500000]\tTrain Loss: 1162.284988671875\tTime: 0:00:01.449698\n",
            "Epoch: [74/100]\tSamples: [370000/500000]\tTrain Loss: 1166.0234369140626\tTime: 0:00:01.533076\n",
            "Epoch: [75/100]\tSamples: [375000/500000]\tTrain Loss: 1164.07932890625\tTime: 0:00:01.552695\n",
            "Epoch: [76/100]\tSamples: [380000/500000]\tTrain Loss: 1157.07339375\tTime: 0:00:01.455950\n",
            "Epoch: [77/100]\tSamples: [385000/500000]\tTrain Loss: 1164.66723046875\tTime: 0:00:01.381123\n",
            "Epoch: [78/100]\tSamples: [390000/500000]\tTrain Loss: 1156.9677294921876\tTime: 0:00:01.363241\n",
            "Epoch: [79/100]\tSamples: [395000/500000]\tTrain Loss: 1161.7733798828126\tTime: 0:00:01.361664\n",
            "Epoch: [80/100]\tSamples: [400000/500000]\tTrain Loss: 1161.57279765625\tTime: 0:00:01.373655\n",
            "Epoch: [81/100]\tSamples: [405000/500000]\tTrain Loss: 1163.578612890625\tTime: 0:00:01.549497\n",
            "Epoch: [82/100]\tSamples: [410000/500000]\tTrain Loss: 1162.9033369140625\tTime: 0:00:01.421113\n",
            "Epoch: [83/100]\tSamples: [415000/500000]\tTrain Loss: 1159.758049609375\tTime: 0:00:01.452402\n",
            "Epoch: [84/100]\tSamples: [420000/500000]\tTrain Loss: 1165.5691974609374\tTime: 0:00:01.393068\n",
            "Epoch: [85/100]\tSamples: [425000/500000]\tTrain Loss: 1163.1632953125\tTime: 0:00:01.542991\n",
            "Epoch: [86/100]\tSamples: [430000/500000]\tTrain Loss: 1162.159598828125\tTime: 0:00:01.360843\n",
            "Epoch: [87/100]\tSamples: [435000/500000]\tTrain Loss: 1165.2721345703126\tTime: 0:00:01.369227\n",
            "Epoch: [88/100]\tSamples: [440000/500000]\tTrain Loss: 1159.988183203125\tTime: 0:00:01.395853\n",
            "Epoch: [89/100]\tSamples: [445000/500000]\tTrain Loss: 1159.1486025390625\tTime: 0:00:01.349172\n",
            "Epoch: [90/100]\tSamples: [450000/500000]\tTrain Loss: 1163.9947984375\tTime: 0:00:01.379920\n",
            "Epoch: [91/100]\tSamples: [455000/500000]\tTrain Loss: 1157.553001171875\tTime: 0:00:01.386009\n",
            "Epoch: [92/100]\tSamples: [460000/500000]\tTrain Loss: 1163.1571857421875\tTime: 0:00:01.391381\n",
            "Epoch: [93/100]\tSamples: [465000/500000]\tTrain Loss: 1161.0759021484375\tTime: 0:00:01.429022\n",
            "Epoch: [94/100]\tSamples: [470000/500000]\tTrain Loss: 1161.3106884765625\tTime: 0:00:01.395379\n",
            "Epoch: [95/100]\tSamples: [475000/500000]\tTrain Loss: 1162.764020703125\tTime: 0:00:01.526993\n",
            "Epoch: [96/100]\tSamples: [480000/500000]\tTrain Loss: 1161.995562109375\tTime: 0:00:01.460212\n",
            "Epoch: [97/100]\tSamples: [485000/500000]\tTrain Loss: 1159.526727734375\tTime: 0:00:01.438195\n",
            "Epoch: [98/100]\tSamples: [490000/500000]\tTrain Loss: 1161.4469587890626\tTime: 0:00:01.544967\n",
            "Epoch: [99/100]\tSamples: [495000/500000]\tTrain Loss: 1161.0009482421874\tTime: 0:00:01.399621\n",
            "Epoch: [100/100]\tSamples: [500000/500000]\tTrain Loss: 1160.3936728515625\tTime: 0:00:01.368157\n"
          ]
        }
      ],
      "source": [
        "train_data_centr, avitm_centr, id2token_centr = train_avitm(documents_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1w7cb9r7QAl",
        "outputId": "416ac711-d0ed-413b-9be3-8707d5eb709a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:03,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic_centr = get_doc_topic_distribution(avitm_centr, train_data_centr, n_samples=5) # get all the topic predictions\n",
        "print(doc_topic_centr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "YHry56Bz7sbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac1817f0-3d32-4125-9bbb-68c43514d63f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "word_topic_centr = convert_topic_word_to_init_size(vocab_size, avitm_centr, n_topics, id2token_centr, all_words)\n",
        "word_topic_centr.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usiERbR-8Roe"
      },
      "source": [
        "# 4. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Ng1ykc1A9wkn"
      },
      "outputs": [],
      "source": [
        "doc_topic_centr_node_0 = doc_topic_centr[0:1000,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lceg9pT2zeP"
      },
      "source": [
        "### Doc-topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF0WXTEy2yu4",
        "outputId": "beaf4078-5d56-4983-b2a7-981b1345defd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference in evaluation of doc similarity (node 0): 692.7350115651025\n",
            "Difference in evaluation of doc similarity (centr): 620.3098986083606\n"
          ]
        }
      ],
      "source": [
        "sim_mat_theoretical = np.sqrt(doc_topics_all[0]).dot(np.sqrt(doc_topics_all[0].T))\n",
        "sim_mat_actual = np.sqrt(doc_topic).dot(np.sqrt(doc_topic.T))\n",
        "print('Difference in evaluation of doc similarity (node 0):', np.sum(np.abs(sim_mat_theoretical - sim_mat_actual))/n_docs)\n",
        "\n",
        "sim_mat_actual_centr = np.sqrt(doc_topic_centr_node_0).dot(np.sqrt(doc_topic_centr_node_0.T))\n",
        "print('Difference in evaluation of doc similarity (centr):', np.sum(np.abs(sim_mat_theoretical - sim_mat_actual_centr))/n_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7WwmitW3VAz"
      },
      "source": [
        "### Topic-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr9sMP2X3b8B",
        "outputId": "188d250b-65d6-4cf8-d188-497d5bddcf68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tópicos (equivalentes) evaluados correctamente (node 0): 5.186504480994585\n",
            "Tópicos (equivalentes) evaluados correctamente (centr): 6.59742516812551\n"
          ]
        }
      ],
      "source": [
        "print('Tópicos (equivalentes) evaluados correctamente (node 0):', np.sum(np.max(np.sqrt(word_topic).dot(np.sqrt(topic_vectors.T)), axis=0)))\n",
        "print('Tópicos (equivalentes) evaluados correctamente (centr):', np.sum(np.max(np.sqrt(word_topic_centr).dot(np.sqrt(topic_vectors.T)), axis=0)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Centralized_ProdLDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}